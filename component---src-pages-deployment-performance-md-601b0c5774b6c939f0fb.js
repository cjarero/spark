(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{jK0j:function(e,t,o){"use strict";o.r(t),o.d(t,"_frontmatter",(function(){return s})),o.d(t,"default",(function(){return c}));var a=o("wx14"),n=o("zLVn"),r=(o("q1tI"),o("7ljp")),i=o("hhGP"),s=(o("qKvR"),{});void 0!==s&&s&&s===Object(s)&&Object.isExtensible(s)&&!s.hasOwnProperty("__filemeta")&&Object.defineProperty(s,"__filemeta",{configurable:!0,value:{name:"_frontmatter",filename:"src/pages/Deployment/Performance.md"}});var l={_frontmatter:s},p=i.a;function c(e){var t=e.components,o=Object(n.a)(e,["components"]);return Object(r.b)(p,Object(a.a)({},l,o,{components:t,mdxType:"MDXLayout"}),Object(r.b)("h1",{id:"performance"},"Performance"),Object(r.b)("p",null,"We have no performance figures of Spark yet. Spark is already being used in production scenarios, so it is fit for real use. If you have measured performance of Spark yourself, please share the results!"),Object(r.b)("p",null,"In the near future we will develop performance tests for Spark to get you an idea whether or how it will fit your performance needs."),Object(r.b)("p",null,"If you are concerned Spark will not handle your load as fast as you would like, consider the following possibilities for spreading the load."),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"If you are FHIR-enabling multiple source systems, you could provide every system with it's own FHIR front-end, implemented by Spark. Instead of feeding data from all the source systems into one instance of Spark. You will however need a way to route requests to the correct instance of Spark.")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"If there is a logical attribute in your data to split the whole set into multiple sets, you could deploy Spark multiple times, each one on a 'shard' of the data. You will need a way to route requests to the correct instance of Spark, based on the chosen attribute.")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"MongoDB supports sharding, as described in the ",Object(r.b)("a",{href:"https://docs.mongodb.com/manual/sharding/",parentName:"p"},"MongoDB documentation"),". You will have to choose a shard key based upon expected use. To use this in Spark, you will probably have to tweak the Spark Mongo implementation."))))}void 0!==c&&c&&c===Object(c)&&Object.isExtensible(c)&&!c.hasOwnProperty("__filemeta")&&Object.defineProperty(c,"__filemeta",{configurable:!0,value:{name:"MDXContent",filename:"src/pages/Deployment/Performance.md"}}),c.isMDXComponent=!0}}]);
//# sourceMappingURL=component---src-pages-deployment-performance-md-601b0c5774b6c939f0fb.js.map