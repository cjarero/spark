(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{Ngqh:function(e,t,a){"use strict";a.r(t),a.d(t,"_frontmatter",(function(){return s})),a.d(t,"default",(function(){return p}));var r=a("wx14"),o=a("zLVn"),n=(a("q1tI"),a("7ljp")),i=a("hhGP"),s=(a("qKvR"),{});void 0!==s&&s&&s===Object(s)&&Object.isExtensible(s)&&!s.hasOwnProperty("__filemeta")&&Object.defineProperty(s,"__filemeta",{configurable:!0,value:{name:"_frontmatter",filename:"src/pages/Deployment/Using_Spark.md"}});var l={_frontmatter:s},u=i.a;function p(e){var t=e.components,a=Object(o.a)(e,["components"]);return Object(n.b)(u,Object(r.a)({},l,a,{components:t,mdxType:"MDXLayout"}),Object(n.b)("p",null,"There are essentially two ways in which you can fit Spark in your own environment. One is using Spark as-is, the other requires you to modify the storage layer."),Object(n.b)("h2",{id:"using-spark-as-is"},"Using Spark as-is"),Object(n.b)("p",null,"In this mode, Spark handles everything FHIR for you, but it requires it's own storage of resources and the search index to do so. This means you have to feed the data that you want to serve as FHIR Resources into the Spark REST interface. So you create a copy of (part of) your data, held in the Spark MongoDB database. These are roughly the steps to follow:"),Object(n.b)("ul",null,Object(n.b)("li",{parentName:"ul"},"Define how the data in your own system(s) maps to FHIR resources. This is the logical mapping."),Object(n.b)("li",{parentName:"ul"},"Create a piece of software that:",Object(n.b)("ul",{parentName:"li"},Object(n.b)("li",{parentName:"ul"},"performs this logical mapping on actual data"),Object(n.b)("li",{parentName:"ul"},"uploads the result of it to Spark (with a FHIR Create operation on the POST ",Object(n.b)("inlineCode",{parentName:"li"},"[base]/fhir/<resourcetype>")," endpoint)\nThe FhirClient from the Hl7.Fhir API is very useful to program this."))),Object(n.b)("li",{parentName:"ul"},"If you need periodic updates from your system to Spark, run the software from the previous step periodically by any means you see fit. It is advised that only updates since the previous run can be recognized in your data, and you feed them into Spark instead of reloading all the data every time.")),Object(n.b)("h2",{id:"using-spark-directly-against-your-own-datastore"},"Using Spark directly against your own datastore"),Object(n.b)("p",null,"In this mode, Spark presents the FHIR REST interface to the outside world, but data retrieval (and eventually storage) is handled by an existing datastore. This requires quite some work in adapting Spark, because you have to replace Spark.Mongo with an implementation targeting your own datastore. Depending on the FHIR interactions that you intend to support, this may or may not be feasible. We think this is a valid option if you:"),Object(n.b)("ul",null,Object(n.b)("li",{parentName:"ul"},"provide read-only access"),Object(n.b)("li",{parentName:"ul"},"to a limited number of Resource types"),Object(n.b)("li",{parentName:"ul"},"supporting only a few search parameters")),Object(n.b)("p",null,"Please be aware that:"),Object(n.b)("ul",null,Object(n.b)("li",{parentName:"ul"},"A FHIR Resource has a server-assigned id, by which it can be retrieved again. Because a resource may span several datastructures in your own datastore, management of these id's is not always straightforward."),Object(n.b)("li",{parentName:"ul"},"A search must be translated to a (usually SQL-) query into your own datastore. Therefore, supporting many search parameters will probably require a lot of work.")),Object(n.b)("p",null,"You will have to do roughly these steps:"),Object(n.b)("ul",null,Object(n.b)("li",{parentName:"ul"},"Define how the data in your own system maps to FHIR resources. This is the logical mapping."),Object(n.b)("li",{parentName:"ul"},"Implement the interfaces regarding storage (see [",Object(n.b)("a",{href:"./architecture",parentName:"li"},"Architecture"),"]. Perform the actual mapping when retrieving the data for a resource from your system."),Object(n.b)("li",{parentName:"ul"},"Adjust the ConformanceBuilder, so the resulting ConformanceStatement states exactly what you support (which Resource types, which operations)."),Object(n.b)("li",{parentName:"ul"},"Use the dependency injection framework to inject your implementations for the storage interfaces (in UnityConfig.cs).")))}void 0!==p&&p&&p===Object(p)&&Object.isExtensible(p)&&!p.hasOwnProperty("__filemeta")&&Object.defineProperty(p,"__filemeta",{configurable:!0,value:{name:"MDXContent",filename:"src/pages/Deployment/Using_Spark.md"}}),p.isMDXComponent=!0}}]);
//# sourceMappingURL=component---src-pages-deployment-using-spark-md-4a0eee04193a65246024.js.map